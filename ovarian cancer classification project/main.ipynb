{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report, RocCurveDisplay, roc_curve,auc, precision_recall_curve, average_precision_score, f1_score, auc\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from scipy.stats import gmean\n",
    "import seaborn as sns\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_theme()\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,30+1):\n",
    "    \n",
    "    X_train = pd.read_csv(f'/camin1/chlee/jupyter/ML project/[24-12-13]/Data/X_train/X_train_{i}.csv')\n",
    "    X_test = pd.read_csv(f'/camin1/chlee/jupyter/ML project/[24-12-13]/Data/X_test/X_test_{i}.csv')    \n",
    "    y_train = pd.read_csv(f'/camin1/chlee/jupyter/ML project/[24-12-13]/Data/y_train/y_train_{i}.csv')\n",
    "    y_test = pd.read_csv(f'/camin1/chlee/jupyter/ML project/[24-12-13]/Data/y_test/y_test_{i}.csv')\n",
    "\n",
    "    X_train = X_train.drop(['FAMILY_HISTORY'], axis=1)\n",
    "    X_test = X_test.drop(['FAMILY_HISTORY'], axis=1)    \n",
    "    \n",
    "    globals()[f'X_train_{i}'] = X_train.drop(X_train.columns[0], axis=1)\n",
    "    globals()[f'X_test_{i}'] = X_test.drop(X_test.columns[0], axis=1)\n",
    "    globals()[f'y_train_{i}'] = y_train.drop(y_train.columns[0], axis=1)\n",
    "    globals()[f'y_test_{i}'] = y_test.drop(y_test.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "for i in range(1,30+1):\n",
    "    globals()[f'fit_LR_{i}'] = pickle.load(open(f\"/camin1/chlee/jupyter/ML project/[24-12-13]/model pkl save/LR/fit_LR_{i}.pkl\", 'rb'))\n",
    "    globals()[f'fit_rf_{i}'] = pickle.load(open(f\"/camin1/chlee/jupyter/ML project/[24-12-13]/model pkl save/RF/fit_rf_{i}.pkl\", 'rb'))\n",
    "    globals()[f'fit_catboost_{i}'] = pickle.load(open(f\"/camin1/chlee/jupyter/ML project/[24-12-13]/model pkl save/Catboost/fit_catboost_{i}.pkl\", 'rb'))\n",
    "    globals()[f'fit_xgb_{i}'] = pickle.load(open(f\"/camin1/chlee/jupyter/ML project/[24-12-13]/model pkl save/XGB/fit_xgb_{i}.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy_LR = []\n",
    "AUROC_LR = []\n",
    "AP_LR = []\n",
    "Sensitivity_LR = []\n",
    "Specificity_LR = []\n",
    "Youden_LR = []\n",
    "f1_LR = []\n",
    "gmean_LR = []\n",
    "\n",
    "\n",
    "LR_fpr_ = []\n",
    "LR_tpr_ = []\n",
    "LR_roc_auc_ = []\n",
    "\n",
    "LR_precision_ = []\n",
    "LR_recall_ = []\n",
    "LR_ap_ = []\n",
    "    \n",
    "for i in range(1,30+1):\n",
    "\n",
    "    exec(f'X_train = X_train_{i}')\n",
    "    exec(f'X_test = X_test_{i}')\n",
    "    \n",
    "    exec(f'y_train = y_train_{i}')\n",
    "    exec(f'y_test = y_test_{i}')\n",
    "\n",
    "    exec(f'model = fit_LR_{i}')\n",
    "\n",
    "    age_mean = np.mean(X_train['AGE'])\n",
    "    age_std = np.std(X_train['AGE'])\n",
    "    \n",
    "    X_train['AGE'] = (X_train['AGE'] - age_mean) / age_std\n",
    "    X_test['AGE'] = (X_test['AGE'] - age_mean) / age_std\n",
    "    \n",
    "    y_pred_probs = model.predict_proba(X_test)[:,1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "        \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1score = f1_score(y_test, y_pred, average='binary')\n",
    "    youden_index = sensitivity + specificity - 1\n",
    "    gmean = geometric_mean_score(y_test, y_pred)\n",
    "\n",
    "    Accuracy_LR.append(accuracy_score(y_test, y_pred))\n",
    "    AUROC_LR.append(roc_auc_score(y_test, y_pred_probs))\n",
    "    AP_LR.append(average_precision_score(y_test, y_pred_probs))\n",
    "    Sensitivity_LR.append(sensitivity)\n",
    "    Specificity_LR.append(specificity)\n",
    "    Youden_LR.append(youden_index)\n",
    "    f1_LR.append(f1score)\n",
    "    gmean_LR.append(gmean)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "\n",
    "    LR_fpr_.append(fpr)\n",
    "    LR_tpr_.append(tpr)\n",
    "    LR_roc_auc_.append(roc_auc)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_probs)\n",
    "    average_precision = average_precision_score(y_test, y_pred_probs)\n",
    "\n",
    "    LR_precision_.append(precision)\n",
    "    LR_recall_.append(recall)\n",
    "    LR_ap_.append(average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_m, acc_h = np.round(mean_confidence_interval(Accuracy_LR),3)\n",
    "spe_m, spe_h = np.round(mean_confidence_interval(Specificity_LR),3)\n",
    "sen_m, sen_h = np.round(mean_confidence_interval(Sensitivity_LR),3)\n",
    "f1_m, f1_h = np.round(mean_confidence_interval(f1_LR),3)\n",
    "auroc_m, auroc_h = np.round(mean_confidence_interval(AUROC_LR),3)\n",
    "ap_m, ap_h = np.round(mean_confidence_interval(AP_LR),3)\n",
    "gmean_m, gmean_h = np.round(mean_confidence_interval(gmean_LR),3)\n",
    "\n",
    "# 신뢰구간\n",
    "print(acc_m,'±', acc_h)\n",
    "print(spe_m,'±', spe_h)\n",
    "print(sen_m,'±', sen_h)\n",
    "print(f1_m,'±', f1_h)\n",
    "print(auroc_m,'±', auroc_h)\n",
    "print(ap_m,'±', ap_h)\n",
    "print(gmean_m,'±', gmean_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy_RF = []\n",
    "AUROC_RF = []\n",
    "AP_RF = []\n",
    "Sensitivity_RF = []\n",
    "Specificity_RF = []\n",
    "Youden_RF = []\n",
    "f1_RF = []\n",
    "gmean_RF = []\n",
    "\n",
    "rf_fpr_ = []\n",
    "rf_tpr_ = []\n",
    "rf_roc_auc_ = []\n",
    "\n",
    "rf_precision_ = []\n",
    "rf_recall_ = []\n",
    "rf_ap_ = []\n",
    "    \n",
    "for i in range(1,30+1):\n",
    "\n",
    "    exec(f'X_train = X_train_{i}')\n",
    "    exec(f'X_test = X_test_{i}')\n",
    "    \n",
    "    exec(f'y_train = y_train_{i}')\n",
    "    exec(f'y_test = y_test_{i}')\n",
    "\n",
    "    exec(f'model = fit_rf_{i}')\n",
    "    \n",
    "    age_mean = np.mean(X_train['AGE'])\n",
    "    age_std = np.std(X_train['AGE'])\n",
    "    \n",
    "    X_train['AGE'] = (X_train['AGE'] - age_mean) / age_std\n",
    "    X_test['AGE'] = (X_test['AGE'] - age_mean) / age_std\n",
    "    \n",
    "    y_pred_probs = model.predict_proba(X_test)[:,1]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "        \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1score = f1_score(y_test, y_pred, average='binary')\n",
    "    youden_index = sensitivity + specificity - 1\n",
    "    gmean = geometric_mean_score(y_test, y_pred)\n",
    "\n",
    "    Accuracy_RF.append(accuracy_score(y_test, y_pred))\n",
    "    AUROC_RF.append(roc_auc_score(y_test, y_pred_probs))\n",
    "    AP_RF.append(average_precision_score(y_test, y_pred_probs))\n",
    "    Sensitivity_RF.append(sensitivity)\n",
    "    Specificity_RF.append(specificity)\n",
    "    Youden_RF.append(youden_index)\n",
    "    f1_RF.append(f1score)\n",
    "    gmean_RF.append(gmean)\n",
    "\n",
    "    # for auroc curve and precision call\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "\n",
    "    rf_fpr_.append(fpr)\n",
    "    rf_tpr_.append(tpr)\n",
    "    rf_roc_auc_.append(roc_auc)\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_probs)\n",
    "    average_precision = average_precision_score(y_test, y_pred_probs)\n",
    "\n",
    "    rf_precision_.append(precision)\n",
    "    rf_recall_.append(recall)\n",
    "    rf_ap_.append(average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_m, acc_h = np.round(mean_confidence_interval(Accuracy_RF),3)\n",
    "spe_m, spe_h = np.round(mean_confidence_interval(Specificity_RF),3)\n",
    "sen_m, sen_h = np.round(mean_confidence_interval(Sensitivity_RF),3)\n",
    "f1_m, f1_h = np.round(mean_confidence_interval(f1_RF),3)\n",
    "auroc_m, auroc_h = np.round(mean_confidence_interval(AUROC_RF),3)\n",
    "ap_m, ap_h = np.round(mean_confidence_interval(AP_RF),3)\n",
    "gmean_m, gmean_h = np.round(mean_confidence_interval(gmean_RF),3)\n",
    "\n",
    "# 신뢰구간\n",
    "print(acc_m,'±', acc_h)\n",
    "print(spe_m,'±', spe_h)\n",
    "print(sen_m,'±', sen_h)\n",
    "print(f1_m,'±', f1_h)\n",
    "print(auroc_m,'±', auroc_h)\n",
    "print(ap_m,'±', ap_h)\n",
    "print(gmean_m,'±', gmean_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy_XGB = []\n",
    "AUROC_XGB = []\n",
    "AP_XGB = []\n",
    "Sensitivity_XGB = []\n",
    "Specificity_XGB = []\n",
    "Youden_XGB = []\n",
    "f1_XGB = []\n",
    "gmean_XGB = []\n",
    "    \n",
    "xgb_fpr_ = []\n",
    "xgb_tpr_ = []\n",
    "xgb_roc_auc_ = []\n",
    "    \n",
    "xgb_precision_ = []\n",
    "xgb_recall_ = []\n",
    "xgb_ap_ = []\n",
    "\n",
    "for i in range(1,30+1):\n",
    "\n",
    "    exec(f'X_train = X_train_{i}')\n",
    "    exec(f'X_test = X_test_{i}')\n",
    "    \n",
    "    exec(f'y_train = y_train_{i}')\n",
    "    exec(f'y_test = y_test_{i}')\n",
    "\n",
    "    exec(f'model = fit_xgb_{i}')\n",
    "    \n",
    "    age_mean = np.mean(X_train['AGE'])\n",
    "    age_std = np.std(X_train['AGE'])\n",
    "    \n",
    "    X_train['AGE'] = (X_train['AGE'] - age_mean) / age_std\n",
    "    X_test['AGE'] = (X_test['AGE'] - age_mean) / age_std\n",
    "    \n",
    "    y_pred_probs = model.predict_proba(X_test)[:,1]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "        \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1score = f1_score(y_test, y_pred, average='binary')\n",
    "    youden_index = sensitivity + specificity - 1\n",
    "    gmean = geometric_mean_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    Accuracy_XGB.append(accuracy_score(y_test, y_pred))\n",
    "    AUROC_XGB.append(roc_auc_score(y_test, y_pred_probs))\n",
    "    AP_XGB.append(average_precision_score(y_test, y_pred_probs))\n",
    "    Sensitivity_XGB.append(sensitivity)\n",
    "    Specificity_XGB.append(specificity)\n",
    "    Youden_XGB.append(youden_index)\n",
    "    f1_XGB.append(f1score)\n",
    "    gmean_XGB.append(gmean)\n",
    "\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "\n",
    "    xgb_fpr_.append(fpr)\n",
    "    xgb_tpr_.append(tpr)\n",
    "    xgb_roc_auc_.append(roc_auc)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_probs)\n",
    "    average_precision = average_precision_score(y_test, y_pred_probs)\n",
    "\n",
    "    xgb_precision_.append(precision)\n",
    "    xgb_recall_.append(recall)\n",
    "    xgb_ap_.append(average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_m, acc_h = np.round(mean_confidence_interval(Accuracy_XGB),3)\n",
    "spe_m, spe_h = np.round(mean_confidence_interval(Specificity_XGB),3)\n",
    "sen_m, sen_h = np.round(mean_confidence_interval(Sensitivity_XGB),3)\n",
    "f1_m, f1_h = np.round(mean_confidence_interval(f1_XGB),3)\n",
    "auroc_m, auroc_h = np.round(mean_confidence_interval(AUROC_XGB),3)\n",
    "ap_m, ap_h = np.round(mean_confidence_interval(AP_XGB),3)\n",
    "gmean_m, gmean_h = np.round(mean_confidence_interval(gmean_XGB),3)\n",
    "\n",
    "# 신뢰구간\n",
    "print(acc_m,'±', acc_h)\n",
    "print(spe_m,'±', spe_h)\n",
    "print(sen_m,'±', sen_h)\n",
    "print(f1_m,'±', f1_h)\n",
    "print(auroc_m,'±', auroc_h)\n",
    "print(ap_m,'±', ap_h)\n",
    "print(gmean_m,'±', gmean_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy_CATBOOST = []\n",
    "AUROC_CATBOOST = []\n",
    "AP_CATBOOST = []\n",
    "Sensitivity_CATBOOST = []\n",
    "Specificity_CATBOOST = []\n",
    "Youden_CATBOOST = []\n",
    "f1_CATBOOST = []\n",
    "gmean_CATBOOST = []\n",
    "\n",
    "catboost_fpr_ = []\n",
    "catboost_tpr_ = []\n",
    "catboost_roc_auc_ = []\n",
    "\n",
    "catboost_precision_ = []\n",
    "catboost_recall_ = []\n",
    "catboost_ap_ = []\n",
    "\n",
    "for i in range(1,30+1):\n",
    "\n",
    "    exec(f'X_train = X_train_{i}')\n",
    "    exec(f'X_test = X_test_{i}')\n",
    "\n",
    "    exec(f'y_train = y_train_{i}')\n",
    "    exec(f'y_test = y_test_{i}')\n",
    "\n",
    "    exec(f'model = fit_catboost_{i}')\n",
    "\n",
    "    age_mean = np.mean(X_train['AGE'])\n",
    "    age_std = np.std(X_train['AGE'])\n",
    "\n",
    "    X_train['AGE'] = (X_train['AGE'] - age_mean) / age_std\n",
    "    X_test['AGE'] = (X_test['AGE'] - age_mean) / age_std\n",
    "\n",
    "    y_pred_probs = model.predict_proba(X_test)[:,1]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1score = f1_score(y_test, y_pred, average='binary')\n",
    "    youden_index = sensitivity + specificity - 1\n",
    "    gmean = geometric_mean_score(y_test, y_pred)\n",
    "\n",
    "    Accuracy_CATBOOST.append(accuracy_score(y_test, y_pred))\n",
    "    AUROC_CATBOOST.append(roc_auc_score(y_test, y_pred_probs))\n",
    "    AP_CATBOOST.append(average_precision_score(y_test, y_pred_probs))\n",
    "    Sensitivity_CATBOOST.append(sensitivity)\n",
    "    Specificity_CATBOOST.append(specificity)\n",
    "    Youden_CATBOOST.append(youden_index)\n",
    "    f1_CATBOOST.append(f1score)\n",
    "    gmean_CATBOOST.append(gmean)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "\n",
    "    catboost_fpr_.append(fpr)\n",
    "    catboost_tpr_.append(tpr)\n",
    "    catboost_roc_auc_.append(roc_auc)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_probs)\n",
    "    average_precision = average_precision_score(y_test, y_pred_probs)\n",
    "\n",
    "    catboost_precision_.append(precision)\n",
    "    catboost_recall_.append(recall)\n",
    "    catboost_ap_.append(average_precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_m, acc_h = np.round(mean_confidence_interval(Accuracy_CATBOOST),3)\n",
    "spe_m, spe_h = np.round(mean_confidence_interval(Specificity_CATBOOST),3)\n",
    "sen_m, sen_h = np.round(mean_confidence_interval(Sensitivity_CATBOOST),3)\n",
    "f1_m, f1_h = np.round(mean_confidence_interval(f1_CATBOOST),3)\n",
    "auroc_m, auroc_h = np.round(mean_confidence_interval(AUROC_CATBOOST),3)\n",
    "ap_m, ap_h = np.round(mean_confidence_interval(AP_CATBOOST),3)\n",
    "gmean_m, gmean_h = np.round(mean_confidence_interval(gmean_CATBOOST),3)\n",
    "\n",
    "# 신뢰구간\n",
    "print(acc_m,'±', acc_h)\n",
    "print(spe_m,'±', spe_h)\n",
    "print(sen_m,'±', sen_h)\n",
    "print(f1_m,'±', f1_h)\n",
    "print(auroc_m,'±', auroc_h)\n",
    "print(ap_m,'±', ap_h)\n",
    "print(gmean_m,'±', gmean_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paired t-test 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "### LR vs. RF ###\n",
    "\n",
    "print(\"### LR vs. RF ###\")\n",
    "print(\"1. Accuracy\")\n",
    "stat, p_val = stats.ttest_rel(Accuracy_LR, Accuracy_RF)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"2. AUROC\")\n",
    "stat, p_val = stats.ttest_rel(AUROC_LR, AUROC_RF)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"3. AP\")\n",
    "stat, p_val = stats.ttest_rel(AP_LR, AP_RF)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"4. Sensitivity\")\n",
    "stat, p_val = stats.ttest_rel(Sensitivity_LR, Sensitivity_RF)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"5. Specificity\")\n",
    "stat, p_val = stats.ttest_rel(Specificity_LR, Specificity_RF)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"6. f1\")\n",
    "stat, p_val = stats.ttest_rel(f1_LR, f1_RF)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"7. g-mean\")\n",
    "stat, p_val = stats.ttest_rel(gmean_LR, gmean_RF)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LR vs. CATBOOST ###\n",
    "\n",
    "print(\"### LR vs. CATBOOST ###\")\n",
    "print(\"1. Accuracy\")\n",
    "stat, p_val = stats.ttest_rel(Accuracy_LR, Accuracy_CATBOOST)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"2. AUROC\")\n",
    "stat, p_val = stats.ttest_rel(AUROC_LR, AUROC_CATBOOST)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"3. AP\")\n",
    "stat, p_val = stats.ttest_rel(AP_LR, AP_CATBOOST)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"4. Sensitivity\")\n",
    "stat, p_val = stats.ttest_rel(Sensitivity_LR, Sensitivity_CATBOOST)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"5. Specificity\")\n",
    "stat, p_val = stats.ttest_rel(Specificity_LR, Specificity_CATBOOST)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"6. f1\")\n",
    "stat, p_val = stats.ttest_rel(f1_LR, f1_CATBOOST)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"7. g-mean\")\n",
    "stat, p_val = stats.ttest_rel(gmean_LR, gmean_CATBOOST)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### LR vs. XGB ###\")\n",
    "print(\"1. Accuracy\")\n",
    "stat, p_val = stats.ttest_rel(Accuracy_LR, Accuracy_XGB)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"2. AUROC\")\n",
    "stat, p_val = stats.ttest_rel(AUROC_LR, AUROC_XGB)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"3. AP\")\n",
    "stat, p_val = stats.ttest_rel(AP_LR, AP_XGB)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"4. Sensitivity\")\n",
    "stat, p_val = stats.ttest_rel(Sensitivity_LR, Sensitivity_XGB)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"5. Specificity\")\n",
    "stat, p_val = stats.ttest_rel(Specificity_LR, Specificity_XGB)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"6. f1\")\n",
    "stat, p_val = stats.ttest_rel(f1_LR, f1_XGB)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))\n",
    "\n",
    "print(\"7. g-mean\")\n",
    "stat, p_val = stats.ttest_rel(gmean_LR, gmean_XGB)\n",
    "print(\"statistic:\", np.round(stat, 4), '  p-value:', np.round(p_val, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mean ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_auroc(model_name):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    for i in range(1,30+1):\n",
    "        \n",
    "        X_train = globals()[f'X_train_{i}']\n",
    "        X_test = globals()[f'X_test_{i}']\n",
    "        \n",
    "        y_train = globals()[f'y_train_{i}']\n",
    "        y_test = globals()[f'y_test_{i}']\n",
    "\n",
    "        classifier = globals()[f'fit_{model_name}_{i}']\n",
    "\n",
    "        age_mean = np.mean(X_train['AGE'])\n",
    "        age_std = np.std(X_train['AGE'])\n",
    "        \n",
    "        X_train['AGE'] = (X_train['AGE'] - age_mean) / age_std\n",
    "        X_test['AGE'] = (X_test['AGE'] - age_mean) / age_std\n",
    "                        \n",
    "        viz = RocCurveDisplay.from_estimator(\n",
    "            classifier,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            ax=ax,)\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    \n",
    "    ax.plot(0)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return mean_fpr, mean_tpr, mean_auc, std_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_mean_fpr, LR_mean_tpr, LR_mean_auc, LR_std_auc =  mean_auroc(model_name='LR')\n",
    "RF_mean_fpr, RF_mean_tpr, RF_mean_auc, RF_std_auc =  mean_auroc(model_name='rf')\n",
    "CATBOOST_mean_fpr, CATBOOST_mean_tpr, CATBOOST_mean_auc, CATBOOST_std_auc =  mean_auroc(model_name='catboost')\n",
    "XGB_mean_fpr, XGB_mean_tpr, XGB_mean_auc, XGB_std_auc =  mean_auroc(model_name='xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'LR':{'mean_fpr': LR_mean_fpr, 'mean_tpr':LR_mean_tpr, 'mean_auc':LR_mean_auc, 'std_auc':LR_std_auc},\n",
    "          'RF':{'mean_fpr': RF_mean_fpr, 'mean_tpr':RF_mean_tpr, 'mean_auc':RF_mean_auc, 'std_auc':RF_std_auc},\n",
    "          'GBM':{'mean_fpr': XGB_mean_fpr, 'mean_tpr':XGB_mean_tpr, 'mean_auc':XGB_mean_auc, 'std_auc':XGB_std_auc},\n",
    "          'Catboost':{'mean_fpr': CATBOOST_mean_fpr, 'mean_tpr':CATBOOST_mean_tpr, 'mean_auc':CATBOOST_mean_auc, 'std_auc':CATBOOST_std_auc}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "\n",
    "for name, values in models.items():\n",
    "    fpr = values['mean_fpr']\n",
    "    tpr = values['mean_tpr']\n",
    "    roc_auc = values['mean_auc']\n",
    "\n",
    "    plt.plot(fpr, tpr, label='%s (AUC = %0.3f)'  % (name, roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "\n",
    "plt.title('ROC curves')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "\n",
    "plt.savefig('/camin1/chlee/jupyter/ML project/[24-12-13]/figure/figure 1.tiff', format='tiff', dpi=300)  \n",
    "plt.savefig('/camin1/chlee/jupyter/ML project/[24-12-13]/figure/figure 1.pdf', format='pdf', dpi=300)  \n",
    "plt.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "\n",
    "for name, values in models.items():\n",
    "    fpr = values['mean_fpr']\n",
    "    tpr = values['mean_tpr']\n",
    "    roc_auc = values['mean_auc']\n",
    "\n",
    "    plt.plot(fpr, tpr, label='%s (AUC = %0.3f)'  % (name, roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "\n",
    "plt.title('ROC curves')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "\n",
    "# plt.savefig('/camin1/chlee/jupyter/ML project/[24-11-22]-3/figure/figure 1.tiff', format='tiff', dpi=300)  \n",
    "# plt.savefig('/camin1/chlee/jupyter/ML project/[24-11-22]-3/figure/figure 1.pdf', format='pdf', dpi=300)  \n",
    "# plt.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ovcancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
